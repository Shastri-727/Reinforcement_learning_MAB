This repository contains implementations and experiments on multi-armed bandit (MAB) algorithms for two classic web applications:

Online Advertisement Placement

Content Personalization

The project compares the performance of several popular bandit algorithms on simulated environments representative of these applications, allowing you to evaluate strategies such as Îµ-Greedy, UCB1, Bayesian UCB, and Thompson Sampling.
